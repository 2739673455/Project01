{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 260, 42450, 2372, 87901, 494, 241665, 6824, 192441, 10271, 8150, 310, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 5.5736, -1.9918,  1.6168, -2.5054, -1.0547, -0.6202,  0.7615, -1.4453],\n",
      "        [-0.0126, -1.2123,  5.7007, -1.3890, -1.8177, -1.8296,  0.3167, -0.7844]]), hidden_states=None, attentions=None)\n",
      "tensor([[9.6867e-01, 5.0182e-04, 1.8524e-02, 3.0026e-04, 1.2809e-03, 1.9780e-03,\n",
      "         7.8759e-03, 8.6677e-04],\n",
      "        [3.2615e-03, 9.8265e-04, 9.8783e-01, 8.2346e-04, 5.3637e-04, 5.3003e-04,\n",
      "         4.5332e-03, 1.5073e-03]])\n",
      "tensor([0, 2])\n",
      "['平淡语调', '开心语调']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Chinese-Emotion-Small\")\n",
    "# 将单个文本传入分词器\n",
    "# tokenizer 返回一个包含三个对象的字典:\n",
    "#  input_ids 是与句子中每个token对应的索引\n",
    "#  attention_mask 指示是否应该关注一个token\n",
    "#  token_type_ids 在存在多个序列时标识一个token属于哪个序列\n",
    "encoded_input = tokenizer(\"刚出锅的馒头一块钱一个！\")\n",
    "print(encoded_input)\n",
    "# 分词器也可以接受列表输入，并对文本进行填充和截断，返回长度统一的批次\n",
    "pt_batch = tokenizer(\n",
    "    [\"今天天气不错\", \"今天天气太好了\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=16,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Chinese-Emotion-Small\")\n",
    "model.eval()\n",
    "# 将输入传入模型，使用**解包字典\n",
    "with torch.no_grad():\n",
    "    pt_outputs = model(**pt_batch)\n",
    "# 模型在 logits 属性输出结果\n",
    "print(pt_outputs)\n",
    "# 在 logits 上应用 softmax 函数来查询概率\n",
    "pt_logits = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "print(pt_logits)\n",
    "# 选取概率最高的标签\n",
    "pt_predictions = pt_logits.argmax(dim=-1)\n",
    "print(pt_predictions)\n",
    "label_mapping = {\n",
    "    0: \"平淡语调\",\n",
    "    1: \"关切语调\",\n",
    "    2: \"开心语调\",\n",
    "    3: \"愤怒语调\",\n",
    "    4: \"悲伤语调\",\n",
    "    5: \"疑问语调\",\n",
    "    6: \"惊奇语调\",\n",
    "    7: \"厌恶语调\",\n",
    "}\n",
    "res = [label_mapping[i.item()] for i in pt_predictions]\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.6.0-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
