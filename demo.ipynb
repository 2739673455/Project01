{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(path=\"csv\", data_files=\"data/news_categorize.csv\")[\"train\"]\n",
    "# 过滤掉过短文本\n",
    "dataset = dataset.filter(lambda x: len(x[\"title\"]) >= 20)\n",
    "dataset = dataset.shuffle().select(range(20000))\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"D:/Code/PythonProject/model/bert-base-chinese\")\n",
    "\n",
    "\n",
    "# 定义预处理函数\n",
    "def preprocess_func(example):\n",
    "    return tokenizer(\n",
    "        example[\"title\"],\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_func, batched=True)\n",
    "encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"])\n",
    "# 创建MLM数据整理器，动态生成掩码和填充\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,  # 掩码比例\n",
    ")\n",
    "# 实例化DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    encoded_dataset[\"train\"],\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    encoded_dataset[\"test\"],\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "# 查看掩蔽后的数据\n",
    "example_batch = next(iter(train_dataloader))\n",
    "for i in range(4):\n",
    "    masked_tokens = tokenizer.convert_ids_to_tokens(example_batch[\"input_ids\"][i])\n",
    "    print(\"\".join(masked_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "# 自定义模型\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "        # 加载bert预训练模型\n",
    "        self.bert = BertModel.from_pretrained(\"D:/Code/PythonProject/model/bert-base-chinese\")\n",
    "        self.linear = nn.Linear(768, output_size)\n",
    "\n",
    "        # 冻结bert所有参数\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        output = self.linear(output.last_hidden_state)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Model(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, test_dataloader, lr, num_epoch, device):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    # CrossEntropyLoss 默认 ignore_index: int = -100，所以不用再设置 ignore_index 参数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        for batch_count, batch in enumerate(train_dataloader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            # 前向传播\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            # 反向传播\n",
    "            loss = criterion(output.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #  梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            if batch_count % 10 == 0:\n",
    "                # labels != -100 为被掩蔽的位置\n",
    "                mask = labels != -100\n",
    "                preds = torch.argmax(output, dim=-1)\n",
    "                accuracy = (preds[mask] == labels[mask]).sum().item() / mask.sum().item()\n",
    "                print(f\"\\repoch:{epoch:0>2}[{'='*(int((batch_count+1) / len(train_dataloader) * 50)):<50}]\", end=\"\")\n",
    "                print(f\" loss:{loss}, accuracy={accuracy}\")\n",
    "        # 模型评估\n",
    "        model.eval()\n",
    "        accuracy_accumulate = 0\n",
    "        sample_count = 0\n",
    "        for batch_count, batch in enumerate(test_dataloader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            # 前向传播\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids, attention_mask, token_type_ids)\n",
    "            # 计算准确率\n",
    "            mask = labels != -100\n",
    "            preds = torch.argmax(output, dim=-1)\n",
    "            this_accuracy = (preds[mask] == labels[mask]).sum().item()\n",
    "            accuracy_accumulate += this_accuracy\n",
    "            sample_count += mask.sum().item()\n",
    "            print(f\"\\r评估：epoch:{epoch:0>2}[{'='*(int((batch_count+1) / len(test_dataloader) * 50)):<50}]\", end=\"\")\n",
    "            print(f\" accuracy={this_accuracy/mask.sum().item()}\", end=\"\")\n",
    "        print(f\"\\naccuracy: {accuracy_accumulate/sample_count}\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 5e-5\n",
    "num_epoch = 2\n",
    "train(model, train_dataloader, test_dataloader, lr, num_epoch, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.6.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
