{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(path=\"parquet\", data_files=\"data/zh_bert_nsp.parquet\")[\"train\"]\n",
    "dataset = dataset.shuffle().select(range(1000))\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "\n",
    "# 定义预处理函数\n",
    "def preprocess_func(example):\n",
    "    encoding = tokenizer(\n",
    "        example[\"sentence1\"],\n",
    "        example[\"sentence2\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "    encoding[\"labels\"] = example[\"label\"]\n",
    "    return encoding\n",
    "\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_func, batched=True)\n",
    "encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "# 实例化DataLoader\n",
    "train_batch_size = 32\n",
    "test_batch_size = 64\n",
    "train_dataloader = DataLoader(encoded_dataset[\"train\"], batch_size=train_batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(encoded_dataset[\"test\"], batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "# 自定义模型\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 加载bert预训练模型\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "\n",
    "        # 冻结bert的所有参数\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        output = self.linear(output.last_hidden_state[:, 0])  # 获取[CLS]对应的隐状态\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, test_dataloader, lr, num_epoch, device):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        for batch_count, batch in enumerate(train_dataloader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            # 前向传播\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            # 反向传播\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #  梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            if batch_count % 10 == 0:\n",
    "                preds = torch.argmax(output, dim=1)\n",
    "                accuracy = (preds == labels).sum().item() / len(labels)\n",
    "                print(f\"\\repoch:{epoch:0>2}[{'='*(int((batch_count+1) / len(train_dataloader) * 50)):<50}]\", end=\"\")\n",
    "                print(f\" loss:{loss}, accuracy={accuracy}\")\n",
    "        # 模型评估\n",
    "        model.eval()\n",
    "        accuracy_accumulate = 0\n",
    "        sample_count = 0\n",
    "        for batch_count, batch in enumerate(test_dataloader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            # 前向传播\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids, attention_mask, token_type_ids)\n",
    "            # 计算准确率\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            this_accuracy = (preds == labels).sum().item()\n",
    "            accuracy_accumulate += this_accuracy\n",
    "            sample_count += len(labels)\n",
    "            print(f\"\\r评估：epoch:{epoch:0>2}[{'='*(int((batch_count+1) / len(test_dataloader) * 50)):<50}]\", end=\"\")\n",
    "            print(f\" accuracy={this_accuracy/len(labels)}\", end=\"\")\n",
    "        print(f\"\\naccuracy: {accuracy_accumulate/sample_count}\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 5e-5\n",
    "num_epoch = 3\n",
    "train(model, train_dataloader, test_dataloader, lr, num_epoch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input):\n",
    "    pt_input = tokenizer(\n",
    "        input[0],\n",
    "        input[1],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(**pt_input)\n",
    "    return output.argmax(dim=-1).item()\n",
    "\n",
    "\n",
    "model.to(\"cpu\")\n",
    "text = [\"春红落尽\", \"夏木成荫\"]\n",
    "res = predict(model, text)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.6.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
